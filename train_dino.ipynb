{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224 , 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diretory = './Data/Training Data modified/Training Data modified/'\n",
    "test_diretory = './Data/Testing Data modified/Testing Data modified/'\n",
    "val_diretory = './Data/Validation Data modified/Vaildation Data modified/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"folders = glob('./Data/Training Data modified/Training Data modified/*')\\nlen(folders)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''folders = glob('./Data/Training Data modified/Training Data modified/*')\n",
    "len(folders)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "ROOT_DIR = os.path.join(cwd, \"Data/Training Data modified/Training Data modified\")\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for folder in os.listdir(ROOT_DIR):\n",
    "    for file in os.listdir(os.path.join(ROOT_DIR, folder)):\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            full_name = os.path.join(ROOT_DIR, folder, file)\n",
    "            labels[full_name] = folder\n",
    "\n",
    "files = labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rokasb/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vits14.to(device)\n",
    "transform_image = T.Compose([T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load an image and return a tensor that can be used as an input to DINOv2.\n",
    "    \"\"\"\n",
    "    img = Image.open(img)\n",
    "\n",
    "    transformed_img = transform_image(img)[:3].unsqueeze(0)\n",
    "\n",
    "    return transformed_img\n",
    "\n",
    "def compute_embeddings(files: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create an index that contains all of the images in the specified list of files.\n",
    "    \"\"\"\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for i, file in enumerate(tqdm(files)):\n",
    "        embeddings = dinov2_vits14(load_image(file).to(device))\n",
    "\n",
    "        all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n",
    "\n",
    "    with open(\"all_embeddings.json\", \"w\") as f:\n",
    "        f.write(json.dumps(all_embeddings))\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323420e45c33498c91cb4b43fb1738a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = compute_embeddings(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[[-0.99110413  2.46310711 -1.04587734 ... -0.41883004 -0.02407139\n",
      "   2.00975561]\n",
      " [ 2.40632749  1.30864286 -3.11871409 ... -0.42658898 -0.69223446\n",
      "   1.81230605]\n",
      " [ 2.85136557  0.01210673 -5.12617064 ...  1.1792742   3.64849257\n",
      "   2.33891892]\n",
      " ...\n",
      " [ 0.58431542  3.15778327  1.39543557 ...  2.14401984 -0.07121381\n",
      "  -0.10946686]\n",
      " [-0.47157219  1.79584062  2.58868456 ...  2.49605751  1.10491323\n",
      "   2.09674072]\n",
      " [-0.79486984  1.39201319  1.4976747  ...  0.91640002  2.38112688\n",
      "   0.81148082]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = [labels[file] for file in files]\n",
    "\n",
    "print(len(embeddings.values()))\n",
    "\n",
    "embedding_list = list(embeddings.values())\n",
    "\n",
    "clf.fit(np.array(embedding_list).reshape(-1, 384), y)\n",
    "\n",
    "print(np.array(embedding_list).reshape(-1, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Cat\n"
     ]
    }
   ],
   "source": [
    "input_file = \"Data/Training Data modified/Training Data modified/Cat/Cat-Train (1).jpg\"\n",
    "\n",
    "new_image = load_image(input_file)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = dinov2_vits14(new_image.to(device))\n",
    "\n",
    "    prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "\n",
    "    print(\"Predicted class: \" + prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9918509895227008\n",
      "859\n"
     ]
    }
   ],
   "source": [
    "paths = glob.glob(\"Data/Testing Data modified/Testing Data modified/*\")\n",
    "\n",
    "correct = 0\n",
    "predicted = 0\n",
    "\n",
    "for path in paths:\n",
    "    label = path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(f\"{path}/*\"):\n",
    "        new_image = load_image(image_path)\n",
    "        with torch.no_grad():\n",
    "            embedding = dinov2_vits14(new_image.to(device))\n",
    "            prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "            predicted += 1\n",
    "            if prediction[0] == label:\n",
    "                correct += 1\n",
    "\n",
    "accuracy = correct / predicted\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  1.0\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "paths = glob.glob(\"Data/Validation Data modified/Validation Data modified/*\")\n",
    "\n",
    "correct = 0\n",
    "predicted = 0\n",
    "\n",
    "for path in paths:\n",
    "    label = path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(f\"{path}/*\"):\n",
    "        new_image = load_image(image_path)\n",
    "        with torch.no_grad():\n",
    "            embedding = dinov2_vits14(new_image.to(device))\n",
    "            prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "            predicted += 1\n",
    "            if prediction[0] == label:\n",
    "                correct += 1\n",
    "\n",
    "accuracy = correct / predicted\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
